{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Loading all the library","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader, random_split\nimport numpy as np\nfrom torch.cuda.amp import autocast, GradScaler\nimport time\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nfrom torchvision.models import resnet18, resnet50","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:33:55.403404Z","iopub.execute_input":"2026-01-24T16:33:55.403720Z","iopub.status.idle":"2026-01-24T16:33:55.408306Z","shell.execute_reply.started":"2026-01-24T16:33:55.403696Z","shell.execute_reply":"2026-01-24T16:33:55.407543Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:33:59.094083Z","iopub.execute_input":"2026-01-24T16:33:59.094363Z","iopub.status.idle":"2026-01-24T16:33:59.098058Z","shell.execute_reply.started":"2026-01-24T16:33:59.094339Z","shell.execute_reply":"2026-01-24T16:33:59.097286Z"}},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":"Random seed for reproducibility","metadata":{}},{"cell_type":"code","source":"torch.manual_seed(42)\nnp.random.seed(42)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:34:01.592169Z","iopub.execute_input":"2026-01-24T16:34:01.592661Z","iopub.status.idle":"2026-01-24T16:34:01.597725Z","shell.execute_reply.started":"2026-01-24T16:34:01.592634Z","shell.execute_reply":"2026-01-24T16:34:01.597197Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"class ResNetFashionMNIST(nn.Module):  # Renamed from ResNetMNIST\n    \"\"\"Wrapper for ResNet to handle 1-channel FashionMNIST images\"\"\"\n    def __init__(self, model_type='resnet18', num_classes=10):\n        super(ResNetFashionMNIST, self).__init__()\n        \n        if model_type == 'resnet18':\n            self.model = resnet18(pretrained=False)\n            # Modify first conv layer for 1-channel input\n            self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        elif model_type == 'resnet50':\n            self.model = resnet50(pretrained=False)\n            self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        \n        # Modify final layer for 10 classes\n        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n    \n    def forward(self, x):\n        return self.model(x)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:34:03.927325Z","iopub.execute_input":"2026-01-24T16:34:03.927636Z","iopub.status.idle":"2026-01-24T16:34:03.933802Z","shell.execute_reply.started":"2026-01-24T16:34:03.927607Z","shell.execute_reply":"2026-01-24T16:34:03.933127Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"def load_fashionmnist_dataset():  # Renamed function\n    \"\"\"Load FashionMNIST dataset only\"\"\"\n    transform = transforms.Compose([\n        transforms.Resize((32, 32)),  # ResNet expects at least 32x32\n        transforms.ToTensor(),\n        transforms.Normalize((0.5,), (0.5,))\n    ])\n    \n    # Only FashionMNIST\n    dataset = torchvision.datasets.FashionMNIST(\n        root='./data', train=True, download=True, transform=transform\n    )\n    test_dataset = torchvision.datasets.FashionMNIST(\n        root='./data', train=False, download=True, transform=transform\n    )\n    \n    # Split train into train and validation (70%-10%-20%)\n    train_size = int(0.7 * len(dataset))\n    val_size = int(0.1 * len(dataset))\n    test_size = len(dataset) - train_size - val_size\n    \n    train_dataset, val_dataset, _ = random_split(\n        dataset, [train_size, val_size, test_size]\n    )\n    \n    return train_dataset, val_dataset, test_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:34:07.389647Z","iopub.execute_input":"2026-01-24T16:34:07.389946Z","iopub.status.idle":"2026-01-24T16:34:07.395657Z","shell.execute_reply.started":"2026-01-24T16:34:07.389923Z","shell.execute_reply":"2026-01-24T16:34:07.394935Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def train_epoch(model, train_loader, optimizer, criterion, device, scaler, use_amp=True):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    running_loss = 0.0\n    all_preds = []\n    all_labels = []\n    \n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        \n        optimizer.zero_grad()\n        \n        if use_amp:\n            with autocast():\n                output = model(data)\n                loss = criterion(output, target)\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n        \n        running_loss += loss.item()\n        _, preds = torch.max(output, 1)\n        all_preds.extend(preds.cpu().numpy())\n        all_labels.extend(target.cpu().numpy())\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    return running_loss / len(train_loader), accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:34:10.957555Z","iopub.execute_input":"2026-01-24T16:34:10.958230Z","iopub.status.idle":"2026-01-24T16:34:10.964233Z","shell.execute_reply.started":"2026-01-24T16:34:10.958204Z","shell.execute_reply":"2026-01-24T16:34:10.963543Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"def validate(model, val_loader, criterion, device, use_amp=True):\n    \"\"\"Validate the model\"\"\"\n    model.eval()\n    val_loss = 0.0\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device), target.to(device)\n            \n            if use_amp:\n                with autocast():\n                    output = model(data)\n                    loss = criterion(output, target)\n            else:\n                output = model(data)\n                loss = criterion(output, target)\n            \n            val_loss += loss.item()\n            _, preds = torch.max(output, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(target.cpu().numpy())\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    return val_loss / len(val_loader), accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:34:14.639879Z","iopub.execute_input":"2026-01-24T16:34:14.640521Z","iopub.status.idle":"2026-01-24T16:34:14.645927Z","shell.execute_reply.started":"2026-01-24T16:34:14.640494Z","shell.execute_reply":"2026-01-24T16:34:14.645183Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"def test(model, test_loader, device, use_amp=True):\n    \"\"\"Test the model\"\"\"\n    model.eval()\n    all_preds = []\n    all_labels = []\n    \n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            \n            if use_amp:\n                with autocast():\n                    output = model(data)\n            else:\n                output = model(data)\n            \n            _, preds = torch.max(output, 1)\n            all_preds.extend(preds.cpu().numpy())\n            all_labels.extend(target.cpu().numpy())\n    \n    accuracy = accuracy_score(all_labels, all_preds)\n    return accuracy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:34:18.864324Z","iopub.execute_input":"2026-01-24T16:34:18.864730Z","iopub.status.idle":"2026-01-24T16:34:18.870469Z","shell.execute_reply.started":"2026-01-24T16:34:18.864701Z","shell.execute_reply":"2026-01-24T16:34:18.869535Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"def run_experiment(model_type, batch_size, optimizer_name, lr, \n                   pin_memory=False, epochs=10, use_amp=True):\n    \"\"\"Run a complete experiment for FashionMNIST\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Load FashionMNIST dataset\n    train_dataset, val_dataset, test_dataset = load_fashionmnist_dataset()\n    \n    # Create data loaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, \n                             pin_memory=pin_memory, num_workers=2)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n                           pin_memory=pin_memory, num_workers=2)\n    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n                            pin_memory=pin_memory, num_workers=2)\n    \n    # Create model\n    model = ResNetFashionMNIST(model_type=model_type, num_classes=10).to(device)\n    \n    # Define loss function\n    criterion = nn.CrossEntropyLoss()\n    \n    # Define optimizer\n    if optimizer_name == 'SGD':\n        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n    else:  # Adam\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    # Mixed precision training\n    scaler = GradScaler() if use_amp else None\n    \n    # Training loop\n    train_times = []\n    epoch_details = []  # Store epoch details\n    \n    for epoch in range(epochs):\n        start_time = time.time()\n        \n        # Train\n        train_loss, train_acc = train_epoch(\n            model, train_loader, optimizer, criterion, device, \n            scaler, use_amp\n        )\n        \n        # Validate\n        val_loss, val_acc = validate(model, val_loader, criterion, device, use_amp)\n        \n        epoch_time = time.time() - start_time\n        train_times.append(epoch_time)\n        \n        # Store epoch details\n        epoch_details.append({\n            'epoch': epoch + 1,\n            'train_loss': train_loss,\n            'train_acc': train_acc * 100,  # Convert to percentage\n            'val_loss': val_loss,\n            'val_acc': val_acc * 100,      # Convert to percentage\n            'time': epoch_time\n        })\n        \n        # Print epoch progress\n        print(f\"  Epoch {epoch+1}/{epochs}: \"\n              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc*100:.2f}%, \"\n              f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc*100:.2f}%, \"\n              f\"Time: {epoch_time:.2f}s\")\n    \n    # Test\n    test_acc = test(model, test_loader, device, use_amp)\n    avg_train_time = np.mean(train_times)\n    \n    # Print test results\n    print(f\"  Test Accuracy: {test_acc*100:.2f}%\")\n    print(f\"  Average training time per epoch: {avg_train_time:.2f}s\")\n    \n    return test_acc * 100, avg_train_time, epoch_details","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:43:49.047048Z","iopub.execute_input":"2026-01-24T16:43:49.047391Z","iopub.status.idle":"2026-01-24T16:43:49.057900Z","shell.execute_reply.started":"2026-01-24T16:43:49.047358Z","shell.execute_reply":"2026-01-24T16:43:49.057214Z"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":"def run_all_experiments():\n    \"\"\"Run all FashionMNIST experiments with epochs 3 and 5 only\"\"\"\n    results = []\n    all_epoch_details = []  # Store all epoch details\n    \n    # Hyperparameter combinations\n    batch_sizes = [16, 32]  # List of batch sizes to iterate over\n    optimizers = ['SGD', 'Adam']\n    learning_rates = [0.001, 0.0001]  \n    models = ['resnet18', 'resnet50']\n    epochs_list = [2, 3]\n    \n    total_experiments = len(batch_sizes) * len(optimizers) * len(learning_rates) * len(models) * len(epochs_list)\n    print(f\"Running FashionMNIST experiments with epochs 3 and 5 only...\")\n    print(f\"Total experiments to run: {total_experiments}\")\n    \n    experiment_count = 0\n    \n    for batch_size in batch_sizes:  # Iterate over each batch size\n        for optimizer_name in optimizers:\n            for lr in learning_rates:\n                for model_type in models:\n                    for epochs in epochs_list:\n                        experiment_count += 1\n                        \n                        print(f\"\\n{'='*70}\")\n                        print(f\"EXPERIMENT {experiment_count}/{total_experiments}\")\n                        print(f\"{'='*70}\")\n                        print(f\"Configuration:\")\n                        print(f\"  Model: {model_type}\")\n                        print(f\"  Batch Size: {batch_size}\")\n                        print(f\"  Optimizer: {optimizer_name}\")\n                        print(f\"  Learning Rate: {lr}\")\n                        print(f\"  Epochs: {epochs}\")\n                        print('='*70)\n                        \n                        try:\n                            pin_memory = torch.cuda.is_available()\n                            \n                            # CORRECTED: Removed dataset_name parameter\n                            test_acc, train_time, epoch_details = run_experiment(\n                                model_type=model_type,\n                                batch_size=batch_size,\n                                optimizer_name=optimizer_name,\n                                lr=lr,\n                                pin_memory=pin_memory,\n                                epochs=epochs,\n                                use_amp=True\n                            )\n                            \n                            # Add epoch details with configuration info\n                            for epoch_detail in epoch_details:\n                                epoch_detail.update({\n                                    'exp_id': experiment_count,\n                                    'model': model_type,\n                                    'batch_size': batch_size,\n                                    'optimizer': optimizer_name,\n                                    'learning_rate': lr,\n                                    'total_epochs': epochs\n                                })\n                                all_epoch_details.append(epoch_detail)\n                            \n                            # Store final results\n                            results.append({\n                                'Dataset': 'FashionMNIST',\n                                'Model': model_type,\n                                'Batch_Size': batch_size,\n                                'Optimizer': optimizer_name,\n                                'Learning_Rate': lr,\n                                'Epochs': epochs,\n                                'Test_Accuracy_%': test_acc,\n                                'Avg_Train_Time_s': train_time\n                            })\n                            \n                            print(f\"\\n✓ Experiment {experiment_count} completed successfully!\")\n                            print(f\"  Final Test Accuracy: {test_acc:.2f}%\")\n                            print(f\"  Avg Time per Epoch: {train_time:.2f}s\")\n                            \n                        except Exception as e:\n                            print(f\"\\n✗ Error in experiment {experiment_count}: {e}\")\n                            import traceback\n                            traceback.print_exc()\n                            continue\n    \n    # Create DataFrames\n    df_results = pd.DataFrame(results)\n    df_epoch_details = pd.DataFrame(all_epoch_details)\n    \n    # Save detailed epoch information\n    df_epoch_details.to_csv('epoch_details.csv', index=False)\n    df_results.to_csv('fashionmnist_results.csv', index=False)\n    \n    # Print summary\n    print(f\"\\n{'='*70}\")\n    print(f\"ALL EXPERIMENTS COMPLETED\")\n    print(f\"{'='*70}\")\n    print(f\"Total successful experiments: {len(results)}/{total_experiments}\")\n    print(f\"Results saved to: fashionmnist_results.csv\")\n    print(f\"Epoch details saved to: epoch_details.csv\")\n    \n    # Display epoch details summary\n    if not df_epoch_details.empty:\n        print(f\"\\n{'='*70}\")\n        print(f\"EPOCH DETAILS SUMMARY\")\n        print(f\"{'='*70}\")\n        \n        # Show first few experiments' epoch details\n        for exp_id in sorted(df_epoch_details['exp_id'].unique())[:3]:  # Show first 3 experiments\n            exp_data = df_epoch_details[df_epoch_details['exp_id'] == exp_id]\n            config = exp_data.iloc[0]\n            \n            print(f\"\\nExperiment {exp_id}: {config['model']}, BS={config['batch_size']}, \"\n                  f\"{config['optimizer']}, LR={config['learning_rate']}\")\n            \n            print(f\"{'Epoch':<6} {'Train Loss':<12} {'Train Acc%':<12} \"\n                  f\"{'Val Loss':<12} {'Val Acc%':<12} {'Time(s)':<10}\")\n            print(f\"{'-'*70}\")\n            \n            for _, row in exp_data.iterrows():\n                print(f\"{row['epoch']:<6} {row['train_loss']:<12.4f} {row['train_acc']:<12.2f} \"\n                      f\"{row['val_loss']:<12.4f} {row['val_acc']:<12.2f} {row['time']:<10.2f}\")\n    \n    return df_results, df_epoch_details\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:44:47.729905Z","iopub.execute_input":"2026-01-24T16:44:47.730531Z","iopub.status.idle":"2026-01-24T16:44:47.742828Z","shell.execute_reply.started":"2026-01-24T16:44:47.730478Z","shell.execute_reply":"2026-01-24T16:44:47.742113Z"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":"# Run all experiments for FashionMNIST\nfashion_results = run_all_experiments()\n\n# Save results to CSV\nfashion_results.to_csv('fashionmnist_results.csv', index=False)\n\n# Display results in a nice table format\nprint(\"\\n\\n\" + \"=\"*100)\nprint(\"FASHIONMNIST EXPERIMENT RESULTS\")\nprint(\"=\"*100)\nprint(fashion_results.to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T16:44:52.412348Z","iopub.execute_input":"2026-01-24T16:44:52.412672Z","iopub.status.idle":"2026-01-24T17:53:27.862361Z","shell.execute_reply.started":"2026-01-24T16:44:52.412645Z","shell.execute_reply":"2026-01-24T17:53:27.861259Z"}},"outputs":[{"name":"stdout","text":"Running FashionMNIST experiments with epochs 3 and 5 only...\nTotal experiments to run: 32\n\n======================================================================\nEXPERIMENT 1/32\n======================================================================\nConfiguration:\n  Model: resnet18\n  Batch Size: 16\n  Optimizer: SGD\n  Learning Rate: 0.001\n  Epochs: 2\n======================================================================\n  Epoch 1/2: Train Loss: 0.5152, Train Acc: 81.56%, Val Loss: 0.3903, Val Acc: 84.67%, Time: 35.64s\n  Epoch 2/2: Train Loss: 0.3472, Train Acc: 87.24%, Val Loss: 0.3188, Val Acc: 88.00%, Time: 36.51s\n  Test Accuracy: 88.19%\n  Average training time per epoch: 36.07s\n\n✓ Experiment 1 completed successfully!\n  Final Test Accuracy: 88.19%\n  Avg Time per Epoch: 36.07s\n\n======================================================================\nEXPERIMENT 2/32\n======================================================================\nConfiguration:\n  Model: resnet18\n  Batch Size: 16\n  Optimizer: SGD\n  Learning Rate: 0.001\n  Epochs: 3\n======================================================================\n  Epoch 1/3: Train Loss: 0.5139, Train Acc: 81.71%, Val Loss: 0.3564, Val Acc: 86.45%, Time: 37.22s\n  Epoch 2/3: Train Loss: 0.3429, Train Acc: 87.34%, Val Loss: 0.3124, Val Acc: 88.48%, Time: 37.10s\n  Epoch 3/3: Train Loss: 0.2872, Train Acc: 89.31%, Val Loss: 0.2865, Val Acc: 89.13%, Time: 34.95s\n  Test Accuracy: 88.48%\n  Average training time per epoch: 36.42s\n\n✓ Experiment 2 completed successfully!\n  Final Test Accuracy: 88.48%\n  Avg Time per Epoch: 36.42s\n\n======================================================================\nEXPERIMENT 3/32\n======================================================================\nConfiguration:\n  Model: resnet50\n  Batch Size: 16\n  Optimizer: SGD\n  Learning Rate: 0.001\n  Epochs: 2\n======================================================================\n  Epoch 1/2: Train Loss: 0.9060, Train Acc: 71.14%, Val Loss: 0.4609, Val Acc: 84.07%, Time: 82.67s\n  Epoch 2/2: Train Loss: 0.4886, Train Acc: 82.91%, Val Loss: 0.3821, Val Acc: 85.87%, Time: 86.36s\n  Test Accuracy: 85.24%\n  Average training time per epoch: 84.51s\n\n✓ Experiment 3 completed successfully!\n  Final Test Accuracy: 85.24%\n  Avg Time per Epoch: 84.51s\n\n======================================================================\nEXPERIMENT 4/32\n======================================================================\nConfiguration:\n  Model: resnet50\n  Batch Size: 16\n  Optimizer: SGD\n  Learning Rate: 0.001\n  Epochs: 3\n======================================================================\n  Epoch 1/3: Train Loss: 0.9238, Train Acc: 70.86%, Val Loss: 0.5080, Val Acc: 82.17%, Time: 85.53s\n  Epoch 2/3: Train Loss: 0.5013, Train Acc: 82.20%, Val Loss: 0.3996, Val Acc: 85.77%, Time: 84.38s\n  Epoch 3/3: Train Loss: 0.4163, Train Acc: 85.01%, Val Loss: 0.3653, Val Acc: 86.10%, Time: 86.16s\n  Test Accuracy: 85.19%\n  Average training time per epoch: 85.36s\n\n✓ Experiment 4 completed successfully!\n  Final Test Accuracy: 85.19%\n  Avg Time per Epoch: 85.36s\n\n======================================================================\nEXPERIMENT 5/32\n======================================================================\nConfiguration:\n  Model: resnet18\n  Batch Size: 16\n  Optimizer: SGD\n  Learning Rate: 0.0001\n  Epochs: 2\n======================================================================\n  Epoch 1/2: Train Loss: 0.6429, Train Acc: 78.00%, Val Loss: 0.3996, Val Acc: 85.57%, Time: 36.39s\n  Epoch 2/2: Train Loss: 0.4104, Train Acc: 85.41%, Val Loss: 0.3495, Val Acc: 87.07%, Time: 36.25s\n  Test Accuracy: 86.25%\n  Average training time per epoch: 36.32s\n\n✓ Experiment 5 completed successfully!\n  Final Test Accuracy: 86.25%\n  Avg Time per Epoch: 36.32s\n\n======================================================================\nEXPERIMENT 6/32\n======================================================================\nConfiguration:\n  Model: resnet18\n  Batch Size: 16\n  Optimizer: SGD\n  Learning Rate: 0.0001\n  Epochs: 3\n======================================================================\n  Epoch 1/3: Train Loss: 0.6370, Train Acc: 78.12%, Val Loss: 0.4002, Val Acc: 86.15%, Time: 36.04s\n  Epoch 2/3: Train Loss: 0.4034, Train Acc: 85.53%, Val Loss: 0.3549, Val Acc: 87.55%, Time: 36.03s\n  Epoch 3/3: Train Loss: 0.3441, Train Acc: 87.70%, Val Loss: 0.3295, Val Acc: 88.28%, Time: 36.49s\n  Test Accuracy: 87.91%\n  Average training time per epoch: 36.18s\n\n✓ Experiment 6 completed successfully!\n  Final Test Accuracy: 87.91%\n  Avg Time per Epoch: 36.18s\n\n======================================================================\nEXPERIMENT 7/32\n======================================================================\nConfiguration:\n  Model: resnet50\n  Batch Size: 16\n  Optimizer: SGD\n  Learning Rate: 0.0001\n  Epochs: 2\n======================================================================\n  Epoch 1/2: Train Loss: 1.0890, Train Acc: 60.49%, Val Loss: 0.6499, Val Acc: 75.83%, Time: 84.05s\n  Epoch 2/2: Train Loss: 0.6815, Train Acc: 75.28%, Val Loss: 0.5626, Val Acc: 79.18%, Time: 87.31s\n  Test Accuracy: 78.50%\n  Average training time per epoch: 85.68s\n\n✓ Experiment 7 completed successfully!\n  Final Test Accuracy: 78.50%\n  Avg Time per Epoch: 85.68s\n\n======================================================================\nEXPERIMENT 8/32\n======================================================================\nConfiguration:\n  Model: resnet50\n  Batch Size: 16\n  Optimizer: SGD\n  Learning Rate: 0.0001\n  Epochs: 3\n======================================================================\n  Epoch 1/3: Train Loss: 1.0955, Train Acc: 60.11%, Val Loss: 0.6434, Val Acc: 76.38%, Time: 83.95s\n  Epoch 2/3: Train Loss: 0.6932, Train Acc: 74.49%, Val Loss: 0.5669, Val Acc: 78.90%, Time: 84.09s\n  Epoch 3/3: Train Loss: 0.6062, Train Acc: 77.61%, Val Loss: 0.5165, Val Acc: 80.82%, Time: 85.11s\n  Test Accuracy: 80.52%\n  Average training time per epoch: 84.38s\n\n✓ Experiment 8 completed successfully!\n  Final Test Accuracy: 80.52%\n  Avg Time per Epoch: 84.38s\n\n======================================================================\nEXPERIMENT 9/32\n======================================================================\nConfiguration:\n  Model: resnet18\n  Batch Size: 16\n  Optimizer: Adam\n  Learning Rate: 0.001\n  Epochs: 2\n======================================================================\n  Epoch 1/2: Train Loss: 0.5210, Train Acc: 81.74%, Val Loss: 0.3931, Val Acc: 85.80%, Time: 38.73s\n  Epoch 2/2: Train Loss: 0.3730, Train Acc: 86.60%, Val Loss: 0.3151, Val Acc: 88.62%, Time: 38.89s\n  Test Accuracy: 88.05%\n  Average training time per epoch: 38.81s\n\n✓ Experiment 9 completed successfully!\n  Final Test Accuracy: 88.05%\n  Avg Time per Epoch: 38.81s\n\n======================================================================\nEXPERIMENT 10/32\n======================================================================\nConfiguration:\n  Model: resnet18\n  Batch Size: 16\n  Optimizer: Adam\n  Learning Rate: 0.001\n  Epochs: 3\n======================================================================\n  Epoch 1/3: Train Loss: 0.5265, Train Acc: 81.44%, Val Loss: 0.4450, Val Acc: 83.52%, Time: 38.47s\n  Epoch 2/3: Train Loss: 0.3714, Train Acc: 86.68%, Val Loss: 0.3359, Val Acc: 88.18%, Time: 39.54s\n  Epoch 3/3: Train Loss: 0.3195, Train Acc: 88.45%, Val Loss: 0.3195, Val Acc: 87.60%, Time: 39.32s\n  Test Accuracy: 87.09%\n  Average training time per epoch: 39.11s\n\n✓ Experiment 10 completed successfully!\n  Final Test Accuracy: 87.09%\n  Avg Time per Epoch: 39.11s\n\n======================================================================\nEXPERIMENT 11/32\n======================================================================\nConfiguration:\n  Model: resnet50\n  Batch Size: 16\n  Optimizer: Adam\n  Learning Rate: 0.001\n  Epochs: 2\n======================================================================\n  Epoch 1/2: Train Loss: 0.8044, Train Acc: 75.06%, Val Loss: 0.5255, Val Acc: 81.08%, Time: 92.28s\n  Epoch 2/2: Train Loss: 0.5370, Train Acc: 82.29%, Val Loss: 0.7997, Val Acc: 76.28%, Time: 92.90s\n  Test Accuracy: 76.15%\n  Average training time per epoch: 92.59s\n\n✓ Experiment 11 completed successfully!\n  Final Test Accuracy: 76.15%\n  Avg Time per Epoch: 92.59s\n\n======================================================================\nEXPERIMENT 12/32\n======================================================================\nConfiguration:\n  Model: resnet50\n  Batch Size: 16\n  Optimizer: Adam\n  Learning Rate: 0.001\n  Epochs: 3\n======================================================================\n  Epoch 1/3: Train Loss: 0.8362, Train Acc: 74.39%, Val Loss: 0.6303, Val Acc: 78.48%, Time: 91.19s\n  Epoch 2/3: Train Loss: 0.5953, Train Acc: 80.48%, Val Loss: 0.6682, Val Acc: 76.35%, Time: 91.69s\n  Epoch 3/3: Train Loss: 0.5152, Train Acc: 82.85%, Val Loss: 0.3708, Val Acc: 86.77%, Time: 93.32s\n  Test Accuracy: 85.69%\n  Average training time per epoch: 92.06s\n\n✓ Experiment 12 completed successfully!\n  Final Test Accuracy: 85.69%\n  Avg Time per Epoch: 92.06s\n\n======================================================================\nEXPERIMENT 13/32\n======================================================================\nConfiguration:\n  Model: resnet18\n  Batch Size: 16\n  Optimizer: Adam\n  Learning Rate: 0.0001\n  Epochs: 2\n======================================================================\n  Epoch 1/2: Train Loss: 0.4978, Train Acc: 82.31%, Val Loss: 0.3521, Val Acc: 86.92%, Time: 38.58s\n  Epoch 2/2: Train Loss: 0.3418, Train Acc: 87.45%, Val Loss: 0.3500, Val Acc: 87.27%, Time: 39.11s\n  Test Accuracy: 87.02%\n  Average training time per epoch: 38.84s\n\n✓ Experiment 13 completed successfully!\n  Final Test Accuracy: 87.02%\n  Avg Time per Epoch: 38.84s\n\n======================================================================\nEXPERIMENT 14/32\n======================================================================\nConfiguration:\n  Model: resnet18\n  Batch Size: 16\n  Optimizer: Adam\n  Learning Rate: 0.0001\n  Epochs: 3\n======================================================================\n  Epoch 1/3: Train Loss: 0.5058, Train Acc: 82.00%, Val Loss: 0.3889, Val Acc: 85.57%, Time: 38.99s\n  Epoch 2/3: Train Loss: 0.3402, Train Acc: 87.66%, Val Loss: 0.3241, Val Acc: 88.07%, Time: 40.03s\n  Epoch 3/3: Train Loss: 0.2871, Train Acc: 89.41%, Val Loss: 0.3091, Val Acc: 89.30%, Time: 38.40s\n  Test Accuracy: 88.46%\n  Average training time per epoch: 39.14s\n\n✓ Experiment 14 completed successfully!\n  Final Test Accuracy: 88.46%\n  Avg Time per Epoch: 39.14s\n\n======================================================================\nEXPERIMENT 15/32\n======================================================================\nConfiguration:\n  Model: resnet50\n  Batch Size: 16\n  Optimizer: Adam\n  Learning Rate: 0.0001\n  Epochs: 2\n======================================================================\n  Epoch 1/2: Train Loss: 0.9147, Train Acc: 67.16%, Val Loss: 0.5697, Val Acc: 79.43%, Time: 93.68s\n  Epoch 2/2: Train Loss: 0.5421, Train Acc: 80.55%, Val Loss: 0.4205, Val Acc: 85.02%, Time: 90.53s\n  Test Accuracy: 84.44%\n  Average training time per epoch: 92.11s\n\n✓ Experiment 15 completed successfully!\n  Final Test Accuracy: 84.44%\n  Avg Time per Epoch: 92.11s\n\n======================================================================\nEXPERIMENT 16/32\n======================================================================\nConfiguration:\n  Model: resnet50\n  Batch Size: 16\n  Optimizer: Adam\n  Learning Rate: 0.0001\n  Epochs: 3\n======================================================================\n  Epoch 1/3: Train Loss: 0.9221, Train Acc: 66.71%, Val Loss: 0.5333, Val Acc: 79.95%, Time: 91.13s\n  Epoch 2/3: Train Loss: 0.5477, Train Acc: 80.47%, Val Loss: 0.4350, Val Acc: 84.40%, Time: 94.88s\n  Epoch 3/3: Train Loss: 0.4490, Train Acc: 83.75%, Val Loss: 0.3958, Val Acc: 85.42%, Time: 94.66s\n  Test Accuracy: 84.70%\n  Average training time per epoch: 93.56s\n\n✓ Experiment 16 completed successfully!\n  Final Test Accuracy: 84.70%\n  Avg Time per Epoch: 93.56s\n\n======================================================================\nEXPERIMENT 17/32\n======================================================================\nConfiguration:\n  Model: resnet18\n  Batch Size: 32\n  Optimizer: SGD\n  Learning Rate: 0.001\n  Epochs: 2\n======================================================================\n  Epoch 1/2: Train Loss: 0.4913, Train Acc: 82.38%, Val Loss: 0.3856, Val Acc: 85.68%, Time: 21.22s\n  Epoch 2/2: Train Loss: 0.3270, Train Acc: 88.04%, Val Loss: 0.3264, Val Acc: 87.90%, Time: 21.26s\n  Test Accuracy: 87.58%\n  Average training time per epoch: 21.24s\n\n✓ Experiment 17 completed successfully!\n  Final Test Accuracy: 87.58%\n  Avg Time per Epoch: 21.24s\n\n======================================================================\nEXPERIMENT 18/32\n======================================================================\nConfiguration:\n  Model: resnet18\n  Batch Size: 32\n  Optimizer: SGD\n  Learning Rate: 0.001\n  Epochs: 3\n======================================================================\n  Epoch 1/3: Train Loss: 0.4988, Train Acc: 82.19%, Val Loss: 0.3793, Val Acc: 85.50%, Time: 20.96s\n  Epoch 2/3: Train Loss: 0.3317, Train Acc: 87.80%, Val Loss: 0.3412, Val Acc: 87.57%, Time: 20.83s\n  Epoch 3/3: Train Loss: 0.2701, Train Acc: 89.98%, Val Loss: 0.3044, Val Acc: 88.38%, Time: 21.25s\n  Test Accuracy: 88.84%\n  Average training time per epoch: 21.01s\n\n✓ Experiment 18 completed successfully!\n  Final Test Accuracy: 88.84%\n  Avg Time per Epoch: 21.01s\n\n======================================================================\nEXPERIMENT 19/32\n======================================================================\nConfiguration:\n  Model: resnet50\n  Batch Size: 32\n  Optimizer: SGD\n  Learning Rate: 0.001\n  Epochs: 2\n======================================================================\n  Epoch 1/2: Train Loss: 0.8520, Train Acc: 70.77%, Val Loss: 0.5348, Val Acc: 79.97%, Time: 49.66s\n  Epoch 2/2: Train Loss: 0.4995, Train Acc: 82.25%, Val Loss: 0.4054, Val Acc: 85.28%, Time: 50.60s\n  Test Accuracy: 84.58%\n  Average training time per epoch: 50.13s\n\n✓ Experiment 19 completed successfully!\n  Final Test Accuracy: 84.58%\n  Avg Time per Epoch: 50.13s\n\n======================================================================\nEXPERIMENT 20/32\n======================================================================\nConfiguration:\n  Model: resnet50\n  Batch Size: 32\n  Optimizer: SGD\n  Learning Rate: 0.001\n  Epochs: 3\n======================================================================\n  Epoch 1/3: Train Loss: 0.8819, Train Acc: 70.08%, Val Loss: 0.6004, Val Acc: 80.38%, Time: 50.58s\n  Epoch 2/3: Train Loss: 0.5379, Train Acc: 80.70%, Val Loss: 0.4544, Val Acc: 82.03%, Time: 48.07s\n  Epoch 3/3: Train Loss: 0.4325, Train Acc: 84.28%, Val Loss: 0.3831, Val Acc: 85.88%, Time: 49.72s\n  Test Accuracy: 85.34%\n  Average training time per epoch: 49.45s\n\n✓ Experiment 20 completed successfully!\n  Final Test Accuracy: 85.34%\n  Avg Time per Epoch: 49.45s\n\n======================================================================\nEXPERIMENT 21/32\n======================================================================\nConfiguration:\n  Model: resnet18\n  Batch Size: 32\n  Optimizer: SGD\n  Learning Rate: 0.0001\n  Epochs: 2\n======================================================================\n  Epoch 1/2: Train Loss: 0.7183, Train Acc: 75.83%, Val Loss: 0.4488, Val Acc: 84.15%, Time: 21.18s\n  Epoch 2/2: Train Loss: 0.4278, Train Acc: 84.60%, Val Loss: 0.3821, Val Acc: 85.88%, Time: 20.41s\n  Test Accuracy: 85.19%\n  Average training time per epoch: 20.80s\n\n✓ Experiment 21 completed successfully!\n  Final Test Accuracy: 85.19%\n  Avg Time per Epoch: 20.80s\n\n======================================================================\nEXPERIMENT 22/32\n======================================================================\nConfiguration:\n  Model: resnet18\n  Batch Size: 32\n  Optimizer: SGD\n  Learning Rate: 0.0001\n  Epochs: 3\n======================================================================\n  Epoch 1/3: Train Loss: 0.6939, Train Acc: 76.76%, Val Loss: 0.4438, Val Acc: 84.02%, Time: 20.22s\n  Epoch 2/3: Train Loss: 0.4216, Train Acc: 84.91%, Val Loss: 0.3912, Val Acc: 86.07%, Time: 21.46s\n  Epoch 3/3: Train Loss: 0.3552, Train Acc: 87.17%, Val Loss: 0.3618, Val Acc: 86.70%, Time: 21.64s\n  Test Accuracy: 86.02%\n  Average training time per epoch: 21.10s\n\n✓ Experiment 22 completed successfully!\n  Final Test Accuracy: 86.02%\n  Avg Time per Epoch: 21.10s\n\n======================================================================\nEXPERIMENT 23/32\n======================================================================\nConfiguration:\n  Model: resnet50\n  Batch Size: 32\n  Optimizer: SGD\n  Learning Rate: 0.0001\n  Epochs: 2\n======================================================================\n  Epoch 1/2: Train Loss: 1.2880, Train Acc: 53.53%, Val Loss: 0.7571, Val Acc: 71.78%, Time: 50.23s\n  Epoch 2/2: Train Loss: 0.7001, Train Acc: 73.95%, Val Loss: 0.6223, Val Acc: 77.32%, Time: 49.32s\n  Test Accuracy: 77.43%\n  Average training time per epoch: 49.77s\n\n✓ Experiment 23 completed successfully!\n  Final Test Accuracy: 77.43%\n  Avg Time per Epoch: 49.77s\n\n======================================================================\nEXPERIMENT 24/32\n======================================================================\nConfiguration:\n  Model: resnet50\n  Batch Size: 32\n  Optimizer: SGD\n  Learning Rate: 0.0001\n  Epochs: 3\n======================================================================\n  Epoch 1/3: Train Loss: 1.2732, Train Acc: 54.13%, Val Loss: 0.7138, Val Acc: 72.40%, Time: 48.04s\n  Epoch 2/3: Train Loss: 0.7117, Train Acc: 73.90%, Val Loss: 0.5729, Val Acc: 78.82%, Time: 47.32s\n  Epoch 3/3: Train Loss: 0.6046, Train Acc: 78.02%, Val Loss: 0.5182, Val Acc: 80.40%, Time: 47.09s\n  Test Accuracy: 79.87%\n  Average training time per epoch: 47.48s\n\n✓ Experiment 24 completed successfully!\n  Final Test Accuracy: 79.87%\n  Avg Time per Epoch: 47.48s\n\n======================================================================\nEXPERIMENT 25/32\n======================================================================\nConfiguration:\n  Model: resnet18\n  Batch Size: 32\n  Optimizer: Adam\n  Learning Rate: 0.001\n  Epochs: 2\n======================================================================\n  Epoch 1/2: Train Loss: 0.4757, Train Acc: 83.18%, Val Loss: 0.3771, Val Acc: 86.63%, Time: 22.03s\n  Epoch 2/2: Train Loss: 0.3440, Train Acc: 87.63%, Val Loss: 0.3355, Val Acc: 87.83%, Time: 22.28s\n  Test Accuracy: 86.99%\n  Average training time per epoch: 22.15s\n\n✓ Experiment 25 completed successfully!\n  Final Test Accuracy: 86.99%\n  Avg Time per Epoch: 22.15s\n\n======================================================================\nEXPERIMENT 26/32\n======================================================================\nConfiguration:\n  Model: resnet18\n  Batch Size: 32\n  Optimizer: Adam\n  Learning Rate: 0.001\n  Epochs: 3\n======================================================================\n  Epoch 1/3: Train Loss: 0.4868, Train Acc: 82.68%, Val Loss: 0.3609, Val Acc: 85.85%, Time: 22.08s\n  Epoch 2/3: Train Loss: 0.3383, Train Acc: 87.92%, Val Loss: 0.2883, Val Acc: 89.37%, Time: 22.69s\n  Epoch 3/3: Train Loss: 0.3002, Train Acc: 89.20%, Val Loss: 0.2892, Val Acc: 89.32%, Time: 22.62s\n  Test Accuracy: 88.68%\n  Average training time per epoch: 22.46s\n\n✓ Experiment 26 completed successfully!\n  Final Test Accuracy: 88.68%\n  Avg Time per Epoch: 22.46s\n\n======================================================================\nEXPERIMENT 27/32\n======================================================================\nConfiguration:\n  Model: resnet50\n  Batch Size: 32\n  Optimizer: Adam\n  Learning Rate: 0.001\n  Epochs: 2\n======================================================================\n  Epoch 1/2: Train Loss: 0.6413, Train Acc: 78.62%, Val Loss: 0.3607, Val Acc: 87.05%, Time: 51.67s\n  Epoch 2/2: Train Loss: 0.5328, Train Acc: 82.94%, Val Loss: 0.4212, Val Acc: 84.68%, Time: 53.24s\n  Test Accuracy: 83.62%\n  Average training time per epoch: 52.45s\n\n✓ Experiment 27 completed successfully!\n  Final Test Accuracy: 83.62%\n  Avg Time per Epoch: 52.45s\n\n======================================================================\nEXPERIMENT 28/32\n======================================================================\nConfiguration:\n  Model: resnet50\n  Batch Size: 32\n  Optimizer: Adam\n  Learning Rate: 0.001\n  Epochs: 3\n======================================================================\n  Epoch 1/3: Train Loss: 0.6620, Train Acc: 77.97%, Val Loss: 0.4320, Val Acc: 85.12%, Time: 50.54s\n  Epoch 2/3: Train Loss: 0.5331, Train Acc: 82.73%, Val Loss: 0.4640, Val Acc: 83.55%, Time: 51.87s\n  Epoch 3/3: Train Loss: 0.4006, Train Acc: 86.08%, Val Loss: 0.3315, Val Acc: 87.93%, Time: 53.37s\n  Test Accuracy: 87.58%\n  Average training time per epoch: 51.92s\n\n✓ Experiment 28 completed successfully!\n  Final Test Accuracy: 87.58%\n  Avg Time per Epoch: 51.92s\n\n======================================================================\nEXPERIMENT 29/32\n======================================================================\nConfiguration:\n  Model: resnet18\n  Batch Size: 32\n  Optimizer: Adam\n  Learning Rate: 0.0001\n  Epochs: 2\n======================================================================\n  Epoch 1/2: Train Loss: 0.4763, Train Acc: 82.90%, Val Loss: 0.3701, Val Acc: 86.05%, Time: 22.37s\n  Epoch 2/2: Train Loss: 0.3162, Train Acc: 88.36%, Val Loss: 0.3448, Val Acc: 87.70%, Time: 22.71s\n  Test Accuracy: 87.20%\n  Average training time per epoch: 22.54s\n\n✓ Experiment 29 completed successfully!\n  Final Test Accuracy: 87.20%\n  Avg Time per Epoch: 22.54s\n\n======================================================================\nEXPERIMENT 30/32\n======================================================================\nConfiguration:\n  Model: resnet18\n  Batch Size: 32\n  Optimizer: Adam\n  Learning Rate: 0.0001\n  Epochs: 3\n======================================================================\n  Epoch 1/3: Train Loss: 0.4903, Train Acc: 82.52%, Val Loss: 0.3518, Val Acc: 87.23%, Time: 22.77s\n  Epoch 2/3: Train Loss: 0.3269, Train Acc: 87.87%, Val Loss: 0.3343, Val Acc: 87.93%, Time: 22.33s\n  Epoch 3/3: Train Loss: 0.2646, Train Acc: 90.23%, Val Loss: 0.3116, Val Acc: 88.63%, Time: 22.10s\n  Test Accuracy: 88.50%\n  Average training time per epoch: 22.40s\n\n✓ Experiment 30 completed successfully!\n  Final Test Accuracy: 88.50%\n  Avg Time per Epoch: 22.40s\n\n======================================================================\nEXPERIMENT 31/32\n======================================================================\nConfiguration:\n  Model: resnet50\n  Batch Size: 32\n  Optimizer: Adam\n  Learning Rate: 0.0001\n  Epochs: 2\n======================================================================\n  Epoch 1/2: Train Loss: 0.9315, Train Acc: 66.36%, Val Loss: 0.6229, Val Acc: 77.00%, Time: 51.74s\n  Epoch 2/2: Train Loss: 0.5446, Train Acc: 80.10%, Val Loss: 0.4646, Val Acc: 83.30%, Time: 51.49s\n  Test Accuracy: 82.23%\n  Average training time per epoch: 51.62s\n\n✓ Experiment 31 completed successfully!\n  Final Test Accuracy: 82.23%\n  Avg Time per Epoch: 51.62s\n\n======================================================================\nEXPERIMENT 32/32\n======================================================================\nConfiguration:\n  Model: resnet50\n  Batch Size: 32\n  Optimizer: Adam\n  Learning Rate: 0.0001\n  Epochs: 3\n======================================================================\n  Epoch 1/3: Train Loss: 0.9142, Train Acc: 66.54%, Val Loss: 0.5656, Val Acc: 79.15%, Time: 51.83s\n  Epoch 2/3: Train Loss: 0.5468, Train Acc: 79.90%, Val Loss: 0.4740, Val Acc: 82.27%, Time: 51.83s\n  Epoch 3/3: Train Loss: 0.4322, Train Acc: 84.36%, Val Loss: 0.4310, Val Acc: 83.80%, Time: 52.43s\n  Test Accuracy: 83.81%\n  Average training time per epoch: 52.03s\n\n✓ Experiment 32 completed successfully!\n  Final Test Accuracy: 83.81%\n  Avg Time per Epoch: 52.03s\n\n======================================================================\nALL EXPERIMENTS COMPLETED\n======================================================================\nTotal successful experiments: 32/32\nResults saved to: fashionmnist_results.csv\nEpoch details saved to: epoch_details.csv\n\n======================================================================\nEPOCH DETAILS SUMMARY\n======================================================================\n\nExperiment 1: resnet18, BS=16, SGD, LR=0.001\nEpoch  Train Loss   Train Acc%   Val Loss     Val Acc%     Time(s)   \n----------------------------------------------------------------------\n1      0.5152       81.56        0.3903       84.67        35.64     \n2      0.3472       87.24        0.3188       88.00        36.51     \n\nExperiment 2: resnet18, BS=16, SGD, LR=0.001\nEpoch  Train Loss   Train Acc%   Val Loss     Val Acc%     Time(s)   \n----------------------------------------------------------------------\n1      0.5139       81.71        0.3564       86.45        37.22     \n2      0.3429       87.34        0.3124       88.48        37.10     \n3      0.2872       89.31        0.2865       89.13        34.95     \n\nExperiment 3: resnet50, BS=16, SGD, LR=0.001\nEpoch  Train Loss   Train Acc%   Val Loss     Val Acc%     Time(s)   \n----------------------------------------------------------------------\n1      0.9060       71.14        0.4609       84.07        82.67     \n2      0.4886       82.91        0.3821       85.87        86.36     \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/1467953513.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Save results to CSV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mfashion_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fashionmnist_results.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Display results in a nice table format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'to_csv'"],"ename":"AttributeError","evalue":"'tuple' object has no attribute 'to_csv'","output_type":"error"}],"execution_count":58},{"cell_type":"code","source":"fashion_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:26:03.314578Z","iopub.execute_input":"2026-01-24T18:26:03.314937Z","iopub.status.idle":"2026-01-24T18:26:03.330334Z","shell.execute_reply.started":"2026-01-24T18:26:03.314904Z","shell.execute_reply":"2026-01-24T18:26:03.329654Z"}},"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"(         Dataset     Model  Batch_Size Optimizer  Learning_Rate  Epochs  \\\n 0   FashionMNIST  resnet18          16       SGD         0.0010       2   \n 1   FashionMNIST  resnet18          16       SGD         0.0010       3   \n 2   FashionMNIST  resnet50          16       SGD         0.0010       2   \n 3   FashionMNIST  resnet50          16       SGD         0.0010       3   \n 4   FashionMNIST  resnet18          16       SGD         0.0001       2   \n 5   FashionMNIST  resnet18          16       SGD         0.0001       3   \n 6   FashionMNIST  resnet50          16       SGD         0.0001       2   \n 7   FashionMNIST  resnet50          16       SGD         0.0001       3   \n 8   FashionMNIST  resnet18          16      Adam         0.0010       2   \n 9   FashionMNIST  resnet18          16      Adam         0.0010       3   \n 10  FashionMNIST  resnet50          16      Adam         0.0010       2   \n 11  FashionMNIST  resnet50          16      Adam         0.0010       3   \n 12  FashionMNIST  resnet18          16      Adam         0.0001       2   \n 13  FashionMNIST  resnet18          16      Adam         0.0001       3   \n 14  FashionMNIST  resnet50          16      Adam         0.0001       2   \n 15  FashionMNIST  resnet50          16      Adam         0.0001       3   \n 16  FashionMNIST  resnet18          32       SGD         0.0010       2   \n 17  FashionMNIST  resnet18          32       SGD         0.0010       3   \n 18  FashionMNIST  resnet50          32       SGD         0.0010       2   \n 19  FashionMNIST  resnet50          32       SGD         0.0010       3   \n 20  FashionMNIST  resnet18          32       SGD         0.0001       2   \n 21  FashionMNIST  resnet18          32       SGD         0.0001       3   \n 22  FashionMNIST  resnet50          32       SGD         0.0001       2   \n 23  FashionMNIST  resnet50          32       SGD         0.0001       3   \n 24  FashionMNIST  resnet18          32      Adam         0.0010       2   \n 25  FashionMNIST  resnet18          32      Adam         0.0010       3   \n 26  FashionMNIST  resnet50          32      Adam         0.0010       2   \n 27  FashionMNIST  resnet50          32      Adam         0.0010       3   \n 28  FashionMNIST  resnet18          32      Adam         0.0001       2   \n 29  FashionMNIST  resnet18          32      Adam         0.0001       3   \n 30  FashionMNIST  resnet50          32      Adam         0.0001       2   \n 31  FashionMNIST  resnet50          32      Adam         0.0001       3   \n \n     Test_Accuracy_%  Avg_Train_Time_s  \n 0             88.19         36.072824  \n 1             88.48         36.424475  \n 2             85.24         84.514917  \n 3             85.19         85.358879  \n 4             86.25         36.320200  \n 5             87.91         36.184623  \n 6             78.50         85.677746  \n 7             80.52         84.382150  \n 8             88.05         38.811414  \n 9             87.09         39.106948  \n 10            76.15         92.586812  \n 11            85.69         92.063114  \n 12            87.02         38.842696  \n 13            88.46         39.141566  \n 14            84.44         92.105099  \n 15            84.70         93.557440  \n 16            87.58         21.238524  \n 17            88.84         21.013942  \n 18            84.58         50.130387  \n 19            85.34         49.453415  \n 20            85.19         20.799352  \n 21            86.02         21.104748  \n 22            77.43         49.774483  \n 23            79.87         47.483035  \n 24            86.99         22.152741  \n 25            88.68         22.463497  \n 26            83.62         52.452728  \n 27            87.58         51.922815  \n 28            87.20         22.537399  \n 29            88.50         22.400053  \n 30            82.23         51.618030  \n 31            83.81         52.032547  ,\n     epoch  train_loss  train_acc  val_loss    val_acc       time  exp_id  \\\n 0       1    0.515248  81.561905  0.390283  84.666667  35.637989       1   \n 1       2    0.347208  87.235714  0.318798  88.000000  36.507659       1   \n 2       1    0.513863  81.714286  0.356399  86.450000  37.219798       2   \n 3       2    0.342900  87.335714  0.312357  88.483333  37.098764       2   \n 4       3    0.287229  89.307143  0.286481  89.133333  34.954864       2   \n ..    ...         ...        ...       ...        ...        ...     ...   \n 75      1    0.931476  66.364286  0.622906  77.000000  51.743811      31   \n 76      2    0.544567  80.095238  0.464557  83.300000  51.492249      31   \n 77      1    0.914249  66.542857  0.565555  79.150000  51.833435      32   \n 78      2    0.546828  79.895238  0.473959  82.266667  51.834066      32   \n 79      3    0.432182  84.361905  0.431030  83.800000  52.430140      32   \n \n        model  batch_size optimizer  learning_rate  total_epochs  \n 0   resnet18          16       SGD         0.0010             2  \n 1   resnet18          16       SGD         0.0010             2  \n 2   resnet18          16       SGD         0.0010             3  \n 3   resnet18          16       SGD         0.0010             3  \n 4   resnet18          16       SGD         0.0010             3  \n ..       ...         ...       ...            ...           ...  \n 75  resnet50          32      Adam         0.0001             2  \n 76  resnet50          32      Adam         0.0001             2  \n 77  resnet50          32      Adam         0.0001             3  \n 78  resnet50          32      Adam         0.0001             3  \n 79  resnet50          32      Adam         0.0001             3  \n \n [80 rows x 12 columns])"},"metadata":{}}],"execution_count":59},{"cell_type":"code","source":"# For the tuple structure you showed\nif isinstance(fashion_results, tuple) and len(fashion_results) >= 2:\n    # Save the summary results (first DataFrame)\n    fashion_results[0].to_csv('fashion_results_summary.csv', index=False)\n    print(\"Saved summary to 'fashion_results_summary.csv'\")\n    \n    # Save the detailed epoch results (second DataFrame)\n    fashion_results[1].to_csv('fashion_results_detailed.csv', index=False)\n    print(\"Saved detailed results to 'fashion_results_detailed.csv'\")\n    \n    # Also create a combined file if you want\n    summary_df = fashion_results[0]\n    detailed_df = fashion_results[1]\n    combined = pd.concat([summary_df, detailed_df], ignore_index=True)\n    combined.to_csv('fashion_results_all.csv', index=False)\n    print(\"Saved combined results to 'fashion_results_all.csv'\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:31:11.183602Z","iopub.execute_input":"2026-01-24T18:31:11.184358Z","iopub.status.idle":"2026-01-24T18:31:11.200063Z","shell.execute_reply.started":"2026-01-24T18:31:11.184329Z","shell.execute_reply":"2026-01-24T18:31:11.199247Z"}},"outputs":[{"name":"stdout","text":"Saved summary to 'fashion_results_summary.csv'\nSaved detailed results to 'fashion_results_detailed.csv'\nSaved combined results to 'fashion_results_all.csv'\n","output_type":"stream"}],"execution_count":62},{"cell_type":"code","source":"import pandas as pd\n\n# Read the CSV file\ndetailed_df = pd.read_csv('fashion_results_detailed.csv')\n\n# Display basic info\nprint(f\"File loaded successfully!\")\nprint(f\"Shape: {detailed_df.shape}\")\nprint(f\"Columns: {list(detailed_df.columns)}\")\n\n# Display the entire DataFrame\nprint(\"\\n=== Complete Data ===\")\nprint(detailed_df)\n\n# Or display with formatting\nprint(\"\\n=== Formatted Display ===\")\nprint(detailed_df.to_string())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-24T18:31:55.914133Z","iopub.execute_input":"2026-01-24T18:31:55.914818Z","iopub.status.idle":"2026-01-24T18:31:55.935984Z","shell.execute_reply.started":"2026-01-24T18:31:55.914790Z","shell.execute_reply":"2026-01-24T18:31:55.935201Z"}},"outputs":[{"name":"stdout","text":"File loaded successfully!\nShape: (80, 12)\nColumns: ['epoch', 'train_loss', 'train_acc', 'val_loss', 'val_acc', 'time', 'exp_id', 'model', 'batch_size', 'optimizer', 'learning_rate', 'total_epochs']\n\n=== Complete Data ===\n    epoch  train_loss  train_acc  val_loss    val_acc       time  exp_id  \\\n0       1    0.515248  81.561905  0.390283  84.666667  35.637989       1   \n1       2    0.347208  87.235714  0.318798  88.000000  36.507659       1   \n2       1    0.513863  81.714286  0.356399  86.450000  37.219798       2   \n3       2    0.342900  87.335714  0.312357  88.483333  37.098764       2   \n4       3    0.287229  89.307143  0.286481  89.133333  34.954864       2   \n..    ...         ...        ...       ...        ...        ...     ...   \n75      1    0.931476  66.364286  0.622906  77.000000  51.743811      31   \n76      2    0.544567  80.095238  0.464557  83.300000  51.492249      31   \n77      1    0.914249  66.542857  0.565555  79.150000  51.833435      32   \n78      2    0.546828  79.895238  0.473959  82.266667  51.834066      32   \n79      3    0.432182  84.361905  0.431030  83.800000  52.430140      32   \n\n       model  batch_size optimizer  learning_rate  total_epochs  \n0   resnet18          16       SGD         0.0010             2  \n1   resnet18          16       SGD         0.0010             2  \n2   resnet18          16       SGD         0.0010             3  \n3   resnet18          16       SGD         0.0010             3  \n4   resnet18          16       SGD         0.0010             3  \n..       ...         ...       ...            ...           ...  \n75  resnet50          32      Adam         0.0001             2  \n76  resnet50          32      Adam         0.0001             2  \n77  resnet50          32      Adam         0.0001             3  \n78  resnet50          32      Adam         0.0001             3  \n79  resnet50          32      Adam         0.0001             3  \n\n[80 rows x 12 columns]\n\n=== Formatted Display ===\n    epoch  train_loss  train_acc  val_loss    val_acc       time  exp_id     model  batch_size optimizer  learning_rate  total_epochs\n0       1    0.515248  81.561905  0.390283  84.666667  35.637989       1  resnet18          16       SGD         0.0010             2\n1       2    0.347208  87.235714  0.318798  88.000000  36.507659       1  resnet18          16       SGD         0.0010             2\n2       1    0.513863  81.714286  0.356399  86.450000  37.219798       2  resnet18          16       SGD         0.0010             3\n3       2    0.342900  87.335714  0.312357  88.483333  37.098764       2  resnet18          16       SGD         0.0010             3\n4       3    0.287229  89.307143  0.286481  89.133333  34.954864       2  resnet18          16       SGD         0.0010             3\n5       1    0.906005  71.142857  0.460871  84.066667  82.669818       3  resnet50          16       SGD         0.0010             2\n6       2    0.488648  82.909524  0.382091  85.866667  86.360016       3  resnet50          16       SGD         0.0010             2\n7       1    0.923790  70.859524  0.508005  82.166667  85.534937       4  resnet50          16       SGD         0.0010             3\n8       2    0.501334  82.197619  0.399562  85.766667  84.379433       4  resnet50          16       SGD         0.0010             3\n9       3    0.416263  85.009524  0.365313  86.100000  86.162267       4  resnet50          16       SGD         0.0010             3\n10      1    0.642893  77.995238  0.399560  85.566667  36.385998       5  resnet18          16       SGD         0.0001             2\n11      2    0.410404  85.407143  0.349547  87.066667  36.254402       5  resnet18          16       SGD         0.0001             2\n12      1    0.636986  78.123810  0.400194  86.150000  36.041677       6  resnet18          16       SGD         0.0001             3\n13      2    0.403361  85.530952  0.354859  87.550000  36.027181       6  resnet18          16       SGD         0.0001             3\n14      3    0.344079  87.697619  0.329544  88.283333  36.485010       6  resnet18          16       SGD         0.0001             3\n15      1    1.089017  60.485714  0.649894  75.833333  84.050317       7  resnet50          16       SGD         0.0001             2\n16      2    0.681459  75.283333  0.562637  79.183333  87.305174       7  resnet50          16       SGD         0.0001             2\n17      1    1.095538  60.109524  0.643380  76.383333  83.950246       8  resnet50          16       SGD         0.0001             3\n18      2    0.693226  74.490476  0.566864  78.900000  84.085977       8  resnet50          16       SGD         0.0001             3\n19      3    0.606236  77.607143  0.516471  80.816667  85.110226       8  resnet50          16       SGD         0.0001             3\n20      1    0.521035  81.740476  0.393138  85.800000  38.732118       9  resnet18          16      Adam         0.0010             2\n21      2    0.372982  86.602381  0.315096  88.616667  38.890709       9  resnet18          16      Adam         0.0010             2\n22      1    0.526491  81.438095  0.445033  83.516667  38.466664      10  resnet18          16      Adam         0.0010             3\n23      2    0.371368  86.676190  0.335882  88.183333  39.537250      10  resnet18          16      Adam         0.0010             3\n24      3    0.319548  88.450000  0.319498  87.600000  39.316929      10  resnet18          16      Adam         0.0010             3\n25      1    0.804406  75.064286  0.525498  81.083333  92.278281      11  resnet50          16      Adam         0.0010             2\n26      2    0.536998  82.288095  0.799651  76.283333  92.895344      11  resnet50          16      Adam         0.0010             2\n27      1    0.836227  74.385714  0.630265  78.483333  91.187915      12  resnet50          16      Adam         0.0010             3\n28      2    0.595335  80.478571  0.668153  76.350000  91.685375      12  resnet50          16      Adam         0.0010             3\n29      3    0.515175  82.854762  0.370759  86.766667  93.316052      12  resnet50          16      Adam         0.0010             3\n30      1    0.497821  82.311905  0.352074  86.916667  38.577591      13  resnet18          16      Adam         0.0001             2\n31      2    0.341839  87.447619  0.349994  87.266667  39.107800      13  resnet18          16      Adam         0.0001             2\n32      1    0.505757  82.002381  0.388896  85.566667  38.993112      14  resnet18          16      Adam         0.0001             3\n33      2    0.340233  87.661905  0.324140  88.066667  40.034269      14  resnet18          16      Adam         0.0001             3\n34      3    0.287090  89.407143  0.309104  89.300000  38.397316      14  resnet18          16      Adam         0.0001             3\n35      1    0.914711  67.157143  0.569680  79.433333  93.679335      15  resnet50          16      Adam         0.0001             2\n36      2    0.542079  80.545238  0.420477  85.016667  90.530863      15  resnet50          16      Adam         0.0001             2\n37      1    0.922100  66.709524  0.533270  79.950000  91.127950      16  resnet50          16      Adam         0.0001             3\n38      2    0.547655  80.473810  0.434999  84.400000  94.882842      16  resnet50          16      Adam         0.0001             3\n39      3    0.448996  83.747619  0.395806  85.416667  94.661527      16  resnet50          16      Adam         0.0001             3\n40      1    0.491329  82.380952  0.385569  85.683333  21.218653      17  resnet18          32       SGD         0.0010             2\n41      2    0.327048  88.035714  0.326363  87.900000  21.258395      17  resnet18          32       SGD         0.0010             2\n42      1    0.498776  82.185714  0.379325  85.500000  20.963201      18  resnet18          32       SGD         0.0010             3\n43      2    0.331704  87.802381  0.341182  87.566667  20.831549      18  resnet18          32       SGD         0.0010             3\n44      3    0.270115  89.980952  0.304415  88.383333  21.247075      18  resnet18          32       SGD         0.0010             3\n45      1    0.852024  70.766667  0.534799  79.966667  49.660321      19  resnet50          32       SGD         0.0010             2\n46      2    0.499481  82.245238  0.405417  85.283333  50.600452      19  resnet50          32       SGD         0.0010             2\n47      1    0.881941  70.083333  0.600434  80.383333  50.577269      20  resnet50          32       SGD         0.0010             3\n48      2    0.537915  80.700000  0.454406  82.033333  48.065525      20  resnet50          32       SGD         0.0010             3\n49      3    0.432545  84.280952  0.383148  85.883333  49.717451      20  resnet50          32       SGD         0.0010             3\n50      1    0.718288  75.826190  0.448772  84.150000  21.184988      21  resnet18          32       SGD         0.0001             2\n51      2    0.427803  84.597619  0.382125  85.883333  20.413715      21  resnet18          32       SGD         0.0001             2\n52      1    0.693925  76.759524  0.443816  84.016667  20.218358      22  resnet18          32       SGD         0.0001             3\n53      2    0.421633  84.911905  0.391229  86.066667  21.458274      22  resnet18          32       SGD         0.0001             3\n54      3    0.355169  87.171429  0.361833  86.700000  21.637613      22  resnet18          32       SGD         0.0001             3\n55      1    1.288034  53.528571  0.757103  71.783333  50.231291      23  resnet50          32       SGD         0.0001             2\n56      2    0.700138  73.954762  0.622274  77.316667  49.317674      23  resnet50          32       SGD         0.0001             2\n57      1    1.273245  54.130952  0.713820  72.400000  48.038351      24  resnet50          32       SGD         0.0001             3\n58      2    0.711686  73.900000  0.572875  78.816667  47.316813      24  resnet50          32       SGD         0.0001             3\n59      3    0.604608  78.016667  0.518199  80.400000  47.093943      24  resnet50          32       SGD         0.0001             3\n60      1    0.475738  83.180952  0.377140  86.633333  22.028661      25  resnet18          32      Adam         0.0010             2\n61      2    0.344024  87.633333  0.335507  87.833333  22.276821      25  resnet18          32      Adam         0.0010             2\n62      1    0.486831  82.678571  0.360945  85.850000  22.075852      26  resnet18          32      Adam         0.0010             3\n63      2    0.338292  87.919048  0.288279  89.366667  22.693250      26  resnet18          32      Adam         0.0010             3\n64      3    0.300170  89.202381  0.289222  89.316667  22.621390      26  resnet18          32      Adam         0.0010             3\n65      1    0.641296  78.619048  0.360652  87.050000  51.667689      27  resnet50          32      Adam         0.0010             2\n66      2    0.532848  82.935714  0.421155  84.683333  53.237766      27  resnet50          32      Adam         0.0010             2\n67      1    0.661986  77.971429  0.431986  85.116667  50.536269      28  resnet50          32      Adam         0.0010             3\n68      2    0.533132  82.726190  0.464020  83.550000  51.866789      28  resnet50          32      Adam         0.0010             3\n69      3    0.400644  86.076190  0.331491  87.933333  53.365386      28  resnet50          32      Adam         0.0010             3\n70      1    0.476260  82.904762  0.370121  86.050000  22.365408      29  resnet18          32      Adam         0.0001             2\n71      2    0.316186  88.364286  0.344838  87.700000  22.709390      29  resnet18          32      Adam         0.0001             2\n72      1    0.490319  82.523810  0.351784  87.233333  22.768953      30  resnet18          32      Adam         0.0001             3\n73      2    0.326922  87.866667  0.334349  87.933333  22.326590      30  resnet18          32      Adam         0.0001             3\n74      3    0.264610  90.228571  0.311553  88.633333  22.104615      30  resnet18          32      Adam         0.0001             3\n75      1    0.931476  66.364286  0.622906  77.000000  51.743811      31  resnet50          32      Adam         0.0001             2\n76      2    0.544567  80.095238  0.464557  83.300000  51.492249      31  resnet50          32      Adam         0.0001             2\n77      1    0.914249  66.542857  0.565555  79.150000  51.833435      32  resnet50          32      Adam         0.0001             3\n78      2    0.546828  79.895238  0.473959  82.266667  51.834066      32  resnet50          32      Adam         0.0001             3\n79      3    0.432182  84.361905  0.431030  83.800000  52.430140      32  resnet50          32      Adam         0.0001             3\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}